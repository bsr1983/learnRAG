"""
Integrated RAG system with all components.
Day 14: ç³»ç»Ÿæ•´åˆ
"""

import sys
import os
from typing import Dict, List
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from retrieval.basic_rag_demo import BasicRAG
from retrieval.rag_fusion_demo import RAGFusion
from llm.structured_output_demo import StructuredOutputDemo

# ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶
load_dotenv(dotenv_path=os.path.join(project_root, '.env'))


class IntegratedRAGSystem:
    """å®Œæ•´çš„ RAG ç³»ç»Ÿï¼šæ•´åˆæ‰€æœ‰ç»„ä»¶"""
    
    def __init__(
        self,
        use_rag_fusion: bool = True,
        use_reranker: bool = True,
        use_structured_output: bool = False,
        llm_provider: str = None  # None è¡¨ç¤ºä»ç¯å¢ƒå˜é‡è¯»å–
    ):
        """
        åˆå§‹åŒ–å®Œæ•´ RAG ç³»ç»Ÿ
        
        Args:
            use_rag_fusion: æ˜¯å¦ä½¿ç”¨ RAG-Fusion
            use_reranker: æ˜¯å¦ä½¿ç”¨é‡æ’
            use_structured_output: æ˜¯å¦ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            llm_provider: LLM æä¾›å•†ï¼ˆdoubao, openai, qwen ç­‰ï¼‰
        """
        self.use_rag_fusion = use_rag_fusion
        self.use_reranker = use_reranker
        self.use_structured_output = use_structured_output
        
        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆä¼ é€’ llm_providerï¼‰
        self.basic_rag = BasicRAG(llm_provider=llm_provider)
        if use_rag_fusion:
            self.rag_fusion = RAGFusion(llm_provider=llm_provider)
        if use_structured_output:
            self.structured_output = StructuredOutputDemo()
        
        # æ ‡è®°æ˜¯å¦å·²æ·»åŠ æ–‡æ¡£ï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
        self._documents_added = False
    
    def add_documents(self, documents: List[str], metadatas: List[Dict] = None, force: bool = False):
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
            force: æ˜¯å¦å¼ºåˆ¶æ·»åŠ ï¼ˆå³ä½¿å·²æ·»åŠ è¿‡ï¼‰
        """
        if self._documents_added and not force:
            print("âš ï¸  æ–‡æ¡£å·²æ·»åŠ è¿‡ï¼Œè·³è¿‡ã€‚å¦‚éœ€é‡æ–°æ·»åŠ ï¼Œè¯·è®¾ç½® force=True æˆ–æ¸…ç†é›†åˆ")
            return
        
        self.basic_rag.add_documents(documents, metadatas)
        self._documents_added = True
    
    def query(
        self,
        query: str,
        return_structured: bool = False,
        output_fields: List[str] = None
    ) -> Dict:
        """
        å®Œæ•´çš„ RAG æŸ¥è¯¢æµç¨‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            return_structured: æ˜¯å¦è¿”å›ç»“æ„åŒ–è¾“å‡º
            output_fields: ç»“æ„åŒ–è¾“å‡ºçš„å­—æ®µåˆ—è¡¨
            
        Returns:
            åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆç­”æ¡ˆçš„å­—å…¸
        """
        # 1. æ£€ç´¢ï¼ˆä½¿ç”¨ RAG-Fusion æˆ–åŸºç¡€ RAGï¼‰
        if self.use_rag_fusion:
            retrieved_docs = self.rag_fusion.retrieve_fusion(
                query,
                num_queries=3,
                top_k_per_query=8,  # å¢åŠ æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢æ•°é‡
                final_top_k=8  # å¢åŠ æœ€ç»ˆè¿”å›æ•°é‡
            )
        else:
            retrieved_docs = self.basic_rag.retrieve(
                query,
                top_k=8,  # å¢åŠ æ£€ç´¢æ•°é‡
                use_reranker=self.use_reranker
            )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå»é‡å¹¶æ ¼å¼åŒ–ï¼‰
        seen_texts = set()
        unique_docs = []
        seen_ids = set()
        
        for doc in retrieved_docs:
            # ä¼˜å…ˆä½¿ç”¨ ID å»é‡ï¼Œå¦‚æœæ²¡æœ‰ ID åˆ™ä½¿ç”¨æ–‡æœ¬å†…å®¹
            doc_id = doc.get("id")
            text = doc.get("text", "").strip()
            
            # å»é‡é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨ IDï¼Œå¦åˆ™ä½¿ç”¨æ–‡æœ¬å†…å®¹
            if doc_id and doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_docs.append(doc)
            elif text and text not in seen_texts:
                seen_texts.add(text)
                unique_docs.append(doc)
        
        # å¦‚æœå»é‡åæ²¡æœ‰æ–‡æ¡£ï¼Œä½¿ç”¨åŸå§‹ç»“æœçš„å‰å‡ ä¸ª
        if not unique_docs:
            print("âš ï¸  è­¦å‘Š: å»é‡åæ²¡æœ‰æ–‡æ¡£ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
            unique_docs = retrieved_docs[:min(5, len(retrieved_docs))]
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
        context_parts = []
        for i, doc in enumerate(unique_docs, 1):
            text = doc.get("text", "").strip()
            if not text:
                continue
            score = doc.get("fusion_score") or doc.get("rerank_score") or doc.get("score", 0)
            context_parts.append(f"æ–‡æ¡£{i}ï¼ˆç›¸å…³åº¦: {score:.3f}ï¼‰: {text}")
        
        context = "\n\n".join(context_parts)
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"\nğŸ“ ä¸Šä¸‹æ–‡æ„å»º:")
        print(f"   æ£€ç´¢æ€»æ•°: {len(retrieved_docs)}")
        print(f"   å»é‡å: {len(unique_docs)} ä¸ªå”¯ä¸€æ–‡æ¡£")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
        
        # å¦‚æœä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œç»™å‡ºæç¤º
        if not context or len(context) < 50:
            print("âš ï¸  è­¦å‘Š: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å†…å®¹è¾ƒå°‘ï¼Œå¯èƒ½å½±å“ç­”æ¡ˆè´¨é‡")
            print(f"   ä¸Šä¸‹æ–‡é¢„è§ˆ: {context[:200]}...")
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        answer = self.basic_rag.generate_answer(query, context)
        
        # 4. ç»“æ„åŒ–è¾“å‡ºï¼ˆå¯é€‰ï¼‰
        structured_data = None
        if return_structured and self.use_structured_output:
            if output_fields:
                structured_data = self.structured_output.simple_extract(
                    answer,
                    output_fields
                )
        
        return {
            "query": query,
            "retrieved_documents": retrieved_docs,
            "context": context,
            "answer": answer,
            "structured_output": structured_data
        }


if __name__ == "__main__":
    # Day 14 ç¤ºä¾‹ï¼šå®Œæ•´ç³»ç»Ÿæ•´åˆ
    print("=" * 50)
    print("Day 14: å®Œæ•´ RAG ç³»ç»Ÿæ•´åˆ")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    llm_provider = os.getenv("LLM_PROVIDER")
    doubao_key = os.getenv("DOUBAO_API_KEY")
    
    if not doubao_key and not os.getenv("OPENAI_API_KEY"):
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° LLM API Key")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("  export DOUBAO_API_KEY=your_key")
        print("  export LLM_PROVIDER=doubao")
        print("æˆ–è€…åˆ›å»º .env æ–‡ä»¶")
        print()
    
    # åˆå§‹åŒ–ç³»ç»Ÿï¼ˆä¼ é€’ llm_providerï¼‰
    system = IntegratedRAGSystem(
        use_rag_fusion=True,
        use_reranker=True,
        use_structured_output=True,
        llm_provider=llm_provider  # ä»ç¯å¢ƒå˜é‡è¯»å–
    )
    
    # å‡†å¤‡æ–‡æ¡£
    documents = [
        "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
        "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºè®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œç†è§£å›¾åƒå’Œè§†é¢‘ä¸­çš„å†…å®¹ã€‚",
        "å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚",
        "ç¥ç»ç½‘ç»œæ˜¯ç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆçš„è®¡ç®—æ¨¡å‹ï¼Œçµæ„Ÿæ¥è‡ªç”Ÿç‰©ç¥ç»ç½‘ç»œã€‚",
        "Transformer æ¶æ„æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ç§é‡è¦æ¨¡å‹æ¶æ„ï¼Œè¢«ç”¨äº BERTã€GPT ç­‰æ¨¡å‹ã€‚"
    ]
    
    # æ·»åŠ æ–‡æ¡£
    print("\næ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“...")
    system.add_documents(documents)
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "è¯·è¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„ä¸»è¦æŠ€æœ¯åˆ†æ”¯"
    
    print(f"\n{'='*50}")
    print(f"æŸ¥è¯¢: {query}")
    print('='*50)
    
    result = system.query(query, return_structured=False)
    
    print("\nã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘")
    # ç»Ÿè®¡å”¯ä¸€æ–‡æ¡£
    unique_texts = set()
    for doc in result["retrieved_documents"]:
        unique_texts.add(doc.get("text", "").strip())
    
    print(f"æ£€ç´¢æ€»æ•°: {len(result['retrieved_documents'])}")
    print(f"å”¯ä¸€æ–‡æ¡£: {len(unique_texts)}")
    print()
    
    # æ˜¾ç¤ºå»é‡åçš„æ–‡æ¡£
    seen_texts = set()
    for i, doc in enumerate(result["retrieved_documents"], 1):
        text = doc.get("text", "").strip()
        if text in seen_texts:
            continue  # è·³è¿‡é‡å¤æ–‡æ¡£
        seen_texts.add(text)
        
        # ä¼˜å…ˆæ˜¾ç¤º fusion_scoreï¼Œç„¶åæ˜¯ rerank_scoreï¼Œæœ€åæ˜¯ score
        if "fusion_score" in doc:
            score = doc["fusion_score"]
            score_type = "èåˆåˆ†æ•°"
        elif "rerank_score" in doc:
            score = doc["rerank_score"]
            score_type = "é‡æ’åˆ†æ•°"
        else:
            score = doc.get("score", 0)
            score_type = "ç›¸ä¼¼åº¦"
        print(f"\n  {len(seen_texts)}. [{score_type}: {score:.4f}]")
        print(f"     {text[:150]}...")
    
    print(f"\nã€ç”Ÿæˆçš„ç­”æ¡ˆã€‘")
    print(f"  {result['answer']}")

